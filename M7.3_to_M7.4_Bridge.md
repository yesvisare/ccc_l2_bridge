# M7.3 → M7.4 BRIDGE SCRIPT: FROM METRICS TO ALERTS

**Bridge Type:** Within-Module (M7.3 → M7.4)  
**Module:** M7 - Distributed Tracing & Advanced Observability  
**Duration:** 3 minutes (~750 words)  
**From:** M7.3 Custom Business Metrics  
**To:** M7.4 Intelligent Alerting

---

## BRIDGE STRUCTURE

### PART 1: RECAP M7.3 — WHAT YOU JUST LEARNED (60 seconds, ~250 words)

**Narrator:**

"[SCREEN: Dashboard showing multiple business metrics — MRR, query success rate, feature adoption, cohort retention]

So you just finished M7.3. You now have custom business metrics tracking what actually matters:

- Monthly Recurring Revenue broken down by subscription tier
- Query success rates showing RAG answer quality
- Feature adoption funnels revealing which features drive engagement
- Cohort retention curves predicting churn before it happens

**You've bridged the gap from 'system works' to 'system delivers value'.**

Your dashboards look great. Your product manager is happy. Your CEO can finally see technical performance translated into business outcomes.

[PAUSE]

But now you have a new problem.

[SCREEN: Alert notification panel — 47 unread alerts, scrolling endlessly]

**Your phone won't stop buzzing.**

- 'Query success rate dropped below 70%' — 6:30 AM
- 'MRR decreased by 3%' — 7:15 AM
- 'Feature adoption for export_pdf dropped 15%' — 8:00 AM
- 'Cohort retention anomaly detected' — 8:30 AM
- 'Query satisfaction below threshold' — 9:00 AM

**That's just this morning. By end of day, you've received 50+ alerts.**

Some are real problems. Most are noise—normal variance, false positives, transient blips.

**The paradox:** You built metrics to see problems earlier. But now you're drowning in alerts and missing the critical ones.

**This is alert fatigue. And it's the #1 reason teams disable business metric alerts entirely.**"

---

### PART 2: THE DRIVING QUESTION (45 seconds, ~188 words)

**Narrator:**

"Here's the core problem:

**Technical metrics are easy to alert on.** Latency >500ms? Alert. Error rate >1%? Alert. Clear thresholds, clear actions.

**Business metrics are different.** They have natural variance:

- Query success rate fluctuates 5-8% day to day (weekends vs. weekdays)
- Feature usage drops 20% during holidays (users on vacation)
- MRR has seasonal patterns (end-of-quarter spikes)

**If you alert on static thresholds, you get:**
- 80% false positives (alerting on normal variance)
- 20% missed critical issues (real problems buried in noise)

**Here's what you need instead:**

1. **Anomaly detection** — alert on statistical deviations, not arbitrary thresholds
2. **Alert aggregation** — group related alerts to reduce noise from 50 to 2
3. **Smart routing** — critical alerts to PagerDuty, warnings to Slack
4. **Auto-remediation** — runbooks that fix common issues automatically

**The goal:** From 50 alerts/day to 2 meaningful alerts/day. From reactive firefighting to proactive problem-solving.

**That's M7.4: Intelligent Alerting.**"

---

### PART 3: PREVIEW M7.4 — WHAT'S COMING (75 seconds, ~312 words)

**Narrator:**

"In M7.4, you're building an intelligent alerting system that adapts to your system's behavior patterns.

### KEY CAPABILITY 1: Statistical Anomaly Detection

[SCREEN: Graph showing baseline with confidence bands]

**Instead of:**
```yaml
alert: QuerySuccessRateLow
expr: query_success_rate < 0.70
```

**You'll build:**
```yaml
alert: QuerySuccessRateAnomaly
expr: |
  abs(query_success_rate - avg_over_time(query_success_rate[7d] offset 7d))
  > 2 * stddev_over_time(query_success_rate[7d] offset 7d)
for: 2h
```

**What this does:** Learns your system's baseline and alerts when metrics deviate significantly—not when they cross arbitrary thresholds.

**Result:** 80% reduction in false positive alerts.

---

### KEY CAPABILITY 2: Alert Aggregation & Deduplication

**The Problem:** When your API gateway goes down, you get:
- 15 alerts for high latency (one per endpoint)
- 8 alerts for increased error rates
- 5 alerts for query success rate drops

**That's 28 alerts for ONE underlying problem.**

**The Solution:** Alert aggregation rules that group related alerts:

```yaml
# Group all alerts caused by API gateway failure
aggregation_rule: api_gateway_degradation
  groups: [latency_high, error_rate_increased, query_success_low]
  root_cause_detection: true
```

**Result:** 28 alerts become 1 grouped alert: "API Gateway Degradation (28 sub-alerts)"

---

### KEY CAPABILITY 3: On-Call Rotation + Smart Routing

**Different alerts need different responses:**

- Critical (P0): Page on-call engineer immediately (PagerDuty)
- Important (P1): Slack alert to engineering channel
- Informational (P2): Log to dashboard, no immediate action

**You'll set up:**
- PagerDuty integration with escalation policies
- Severity-based routing (P0 → page, P1 → Slack, P2 → log)
- Business hours vs. after-hours rules

---

### KEY CAPABILITY 4: Runbook Automation

**For common issues**, automate the fix:

**Example: High cache miss rate → Clear cache**

```python
if alert == "cache_miss_rate_high":
    run_runbook("clear_redis_cache")
    if metric_improved_within_5min:
        auto_resolve_alert()
    else:
        escalate_to_oncall()
```

**Result:** 40% of alerts auto-remediate without human intervention.

---

**By end of M7.4:**

- 50 alerts/day → 2 meaningful alerts/day
- 80% false positives → <10% false positives
- 2 hours/day managing alerts → 15 minutes/day
- Reactive firefighting → Proactive health monitoring

**Ready? Let's build it."**

---

**END OF BRIDGE SCRIPT**

**Total Word Count:** ~750 words (~3 minutes at 250 wpm)

---

## PRODUCTION NOTES

**Visuals Needed:**

1. Dashboard full of business metrics (M7.3 recap)
2. Phone screen with 47+ alert notifications (alert fatigue visual)
3. Graph showing natural metric variance (why thresholds fail)
4. Anomaly detection visualization (baseline + confidence bands)
5. Alert aggregation diagram (28 alerts → 1 grouped alert)
6. Severity-based routing flowchart (P0 → PagerDuty, P1 → Slack, P2 → Log)

**Pacing:**

- Part 1 (Recap): Quick, energetic — set up the problem
- Part 2 (Driving Question): Problem-focused, building tension
- Part 3 (Preview): Solution-oriented, excitement for next video

**Tone:** Empathetic about alert fatigue (this is a universal pain point), then optimistic about solutions.

**Key Transition Moment:**

The shift from "you built great metrics" to "but now you're drowning in alerts" should feel relatable—this is a natural consequence of adding more monitoring, not a failure.
