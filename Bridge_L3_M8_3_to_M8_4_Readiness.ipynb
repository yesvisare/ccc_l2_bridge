{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M8.3 ‚Üí L3.M8.4: From Automation to Human Validation\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "M8.3 delivered automated regression testing with CI/CD pipelines maintaining high RAGAS scores (0.92 faithfulness). However, user satisfaction dropped from 82% to 67% despite excellent automated metrics. This bridge validates readiness to shift from **pure automation** to **human-in-the-loop evaluation** that captures subjective quality dimensions (tone, structure, clarity) automated metrics cannot measure.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts Covered\n",
    "\n",
    "- **Metric divergence:** When automated scores and user satisfaction decouple\n",
    "- **Human annotation costs:** Budgeting for crowdsourced ($25-100) vs expert ($250-750) labeling per 100 queries\n",
    "- **Inter-annotator agreement:** Accepting 70-85% consensus (not 100% like automated tests)\n",
    "- **Active learning priorities:** Identifying high-RAGAS/low-satisfaction blind spots\n",
    "\n",
    "---\n",
    "\n",
    "## After Completing\n",
    "\n",
    "You will be able to:\n",
    "- Diagnose when automated evaluation fails to predict user experience\n",
    "- Calculate annotation budgets based on query volume and expertise needs\n",
    "- Design feedback collection that captures subjective quality dimensions\n",
    "- Justify when human labeling is unnecessary (RAGAS ‚Üî satisfaction correlation > 0.90)\n",
    "\n",
    "---\n",
    "\n",
    "## Context in Track\n",
    "\n",
    "**Bridge:** L3.M8.3 ‚Üí L3.M8.4  \n",
    "**Duration:** 8-10 minutes  \n",
    "**Prerequisites:** Completed M8.3 (Regression Testing & CI/CD with RAGAS baselines)\n",
    "\n",
    "---\n",
    "\n",
    "## Run Locally (Windows)\n",
    "\n",
    "```powershell\n",
    "powershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook Bridge_L3_M8_3_to_M8_4_Readiness.ipynb\"\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "jupyter notebook Bridge_L3_M8_3_to_M8_4_Readiness.ipynb\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: What M8.3 Delivered\n",
    "\n",
    "**M8.3 Achievements:**\n",
    "- ‚úÖ **Automated Quality Gates** via GitHub Actions\n",
    "- ‚úÖ **Fast Test Execution** (<5 min CI/CD pipeline)\n",
    "- ‚úÖ **Model Versioning** with DVC\n",
    "- ‚úÖ **Zero Regressions** reaching production over 3 weeks\n",
    "- ‚úÖ **0.87 Faithfulness Baseline** maintained\n",
    "\n",
    "**The Problem Uncovered:**\n",
    "> RAGAS metrics showed excellent scores (0.92 faithfulness, 0.88 relevance)  \n",
    "> BUT user satisfaction dropped from 82% ‚Üí 67% positive\n",
    "\n",
    "**Core Issue:** Automated metrics measure **WHAT** answers contain, but miss **HOW** answers are delivered (tone, structure, clarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Readiness Check #1: Production RAG with Tracking\n",
    "\n",
    "Verify your system tracks >100 queries/week with user IDs and feedback storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for query logs, feedback database, or analytics configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "artifacts = {\n",
    "    \"query_logs\": Path(\"query_logs\").exists(),\n",
    "    \"feedback_db\": Path(\"user_feedback.db\").exists(),\n",
    "    \"analytics_config\": Path(\"config/analytics.yaml\").exists()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Production Tracking Status:\")\n",
    "for name, exists in artifacts.items():\n",
    "    status = \"‚úÖ FOUND\" if exists else \"‚ö†Ô∏è  MISSING\"\n",
    "    print(f\"  {status}: {name}\")\n",
    "\n",
    "# Expected: At least 1 artifact exists; if none, this is a learning exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Readiness Check #2: RAGAS Baseline Established\n",
    "\n",
    "Identify divergence between automated scores and user satisfaction. If correlation > 0.90, human labeling may be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics to detect RAGAS vs satisfaction divergence\n",
    "import json\n",
    "\n",
    "metrics_file = Path(\"metrics.json\")\n",
    "\n",
    "if metrics_file.exists():\n",
    "    try:\n",
    "        data = json.loads(metrics_file.read_text())\n",
    "        print(f\"RAGAS Faithfulness: {data.get('faithfulness', 'N/A')}\")\n",
    "        print(f\"User Satisfaction: {data.get('satisfaction', 'N/A')}\")\n",
    "        print(f\"Correlation: {data.get('correlation', 'N/A')}\")\n",
    "    except (json.JSONDecodeError, IOError):\n",
    "        print(\"‚ö†Ô∏è Skipping (metrics.json exists but unreadable)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping (no metrics.json found)\")\n",
    "    print(\"Expected: RAGAS > 0.85 but satisfaction < 0.75 indicates divergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Readiness Check #3: Budget Awareness\n",
    "\n",
    "Calculate annotation costs for crowdsourced ($25-100) vs expert ($250-750) labeling per 100 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget planning for human annotation\n",
    "queries_per_week = 100  # From requirement: >100 queries/week\n",
    "weeks_to_label = 4      # Initial labeling period\n",
    "\n",
    "crowdsourced_cost = (25, 100)  # min, max per 100 queries\n",
    "expert_cost = (250, 750)\n",
    "\n",
    "total_queries = queries_per_week * weeks_to_label\n",
    "batches = total_queries / 100\n",
    "\n",
    "print(f\"üìä Budget Estimate for {total_queries} queries ({weeks_to_label} weeks):\")\n",
    "print(f\"  Crowdsourced: ${batches * crowdsourced_cost[0]:.0f} - ${batches * crowdsourced_cost[1]:.0f}\")\n",
    "print(f\"  Expert:       ${batches * expert_cost[0]:.0f} - ${batches * expert_cost[1]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Readiness Check #4: Embrace Ambiguity\n",
    "\n",
    "Human annotation yields 70-85% inter-annotator agreement, not the 100% reproducibility of automated tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate inter-annotator agreement on 3 sample queries\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "annotators = [\"Annotator_A\", \"Annotator_B\", \"Annotator_C\"]\n",
    "\n",
    "print(\"üìä Sample Inter-Annotator Agreement:\")\n",
    "for q in range(3):\n",
    "    ratings = [random.randint(3, 5) for _ in annotators]\n",
    "    agreement = len(set(ratings)) == 1\n",
    "    print(f\"  Query {q+1}: {ratings} - {'‚úÖ Agree' if agreement else '‚ö†Ô∏è  Disagree'}\")\n",
    "\n",
    "print(\"\\nExpected: 70-85% agreement is normal (not 100%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Call-Forward: What M8.4 Will Introduce\n",
    "\n",
    "**M8.4 Focus:** Human-in-the-Loop Evaluation\n",
    "\n",
    "### Four Key Capabilities:\n",
    "\n",
    "#### 1. Feedback Collection\n",
    "- Detailed ratings: **Accuracy, Clarity, Helpfulness** (1-5 scales)\n",
    "- Goes beyond binary thumbs-up/down\n",
    "- Captures subjective dimensions automation misses\n",
    "\n",
    "#### 2. Active Learning\n",
    "Prioritize high-value queries:\n",
    "- **Blind spots** (high-RAGAS, low-satisfaction)\n",
    "- **Borderline RAGAS** scores\n",
    "- **New patterns** not in test sets\n",
    "- **High-stakes domains** (legal, medical)\n",
    "\n",
    "#### 3. Label Studio Integration\n",
    "- Structured annotation workflows\n",
    "- Multi-annotator support\n",
    "- Quality control mechanisms\n",
    "- Export to training datasets\n",
    "\n",
    "#### 4. Closed-Loop Improvement\n",
    "Use human labels to:\n",
    "- Refine prompts\n",
    "- Expand test sets\n",
    "- Retrain embeddings\n",
    "- Update evaluation criteria\n",
    "\n",
    "---\n",
    "\n",
    "### Critical Insight\n",
    "\n",
    "> **\"You can have 0.92 faithfulness and still ship unhelpful answers‚Äîbecause automated evaluation doesn't capture user experience.\"**\n",
    "\n",
    "**When NOT to use human labeling:**\n",
    "- If RAGAS ‚Üî satisfaction correlation > 0.90, automation is sufficient\n",
    "\n",
    "**PractaThon Exercise (30 min):**\n",
    "Create `automation_gaps.md` documenting:\n",
    "- High-RAGAS/Low-satisfaction cases\n",
    "- Failure categories: Tone (35%), Structure (40%), Context (15%), Interpretation (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `automation_gaps.md` stub for documenting cases where automated metrics missed quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create automation_gaps.md template for PractaThon exercise\n",
    "stub_content = \"\"\"# Automation Gaps Analysis\n",
    "\n",
    "## High-RAGAS / Low-Satisfaction Cases\n",
    "\n",
    "### Case 1: [Query ID]\n",
    "- **RAGAS Score:** 0.92\n",
    "- **User Satisfaction:** 2/5\n",
    "- **Issue Category:** Tone (35%)\n",
    "- **Description:** [Technically correct but confusing delivery]\n",
    "\n",
    "### Case 2: [Query ID]\n",
    "- **RAGAS Score:** 0.89\n",
    "- **User Satisfaction:** 3/5\n",
    "- **Issue Category:** Structure (40%)\n",
    "- **Description:** [Verbose, lacking conciseness]\n",
    "\n",
    "## Failure Category Distribution\n",
    "- **Tone:** 35%\n",
    "- **Structure:** 40%\n",
    "- **Context:** 15%\n",
    "- **Interpretation:** 10%\n",
    "\n",
    "## Next Steps\n",
    "1. Annotate 100 queries using Label Studio\n",
    "2. Calculate inter-annotator agreement\n",
    "3. Update prompt templates based on findings\n",
    "\"\"\"\n",
    "\n",
    "output_file = Path(\"automation_gaps.md\")\n",
    "\n",
    "try:\n",
    "    output_file.write_text(stub_content)\n",
    "    print(f\"‚úÖ Created: {output_file}\")\n",
    "except (IOError, OSError) as e:\n",
    "    print(f\"‚ö†Ô∏è Skipping file creation (offline/permission issue): {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
