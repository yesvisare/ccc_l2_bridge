{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M8.3 â†’ L3.M8.4: From Automation to Human Validation\n",
    "\n",
    "**Duration:** 8-10 minutes  \n",
    "**Purpose:** Validate readiness to transition from automated testing to human-in-the-loop evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: What M8.3 Delivered\n",
    "\n",
    "**M8.3 Achievements:**\n",
    "- âœ… **Automated Quality Gates** via GitHub Actions\n",
    "- âœ… **Fast Test Execution** (<5 min CI/CD pipeline)\n",
    "- âœ… **Model Versioning** with DVC\n",
    "- âœ… **Zero Regressions** reaching production over 3 weeks\n",
    "- âœ… **0.87 Faithfulness Baseline** maintained\n",
    "\n",
    "**The Problem Uncovered:**\n",
    "> RAGAS metrics showed excellent scores (0.92 faithfulness, 0.88 relevance)  \n",
    "> BUT user satisfaction dropped from 82% â†’ 67% positive\n",
    "\n",
    "**Core Issue:** Automated metrics measure **WHAT** answers contain, but miss **HOW** answers are delivered (tone, structure, clarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2. Readiness Check #1: Production RAG with Tracking\n\n**Requirement:** >100 queries/week with user IDs and feedback storage\n\n**What to verify:**\n- Query logs exist\n- User identifiers captured\n- Feedback mechanism in place",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #1: Production RAG with Tracking\nimport os\nfrom pathlib import Path\n\n# Expected: query_logs/, user_feedback.db, or analytics service config\nartifacts = {\n    \"query_logs\": Path(\"query_logs\").exists(),\n    \"feedback_db\": Path(\"user_feedback.db\").exists(),\n    \"analytics_config\": Path(\"config/analytics.yaml\").exists()\n}\n\nprint(\"âœ… Production Tracking Status:\")\nfor name, exists in artifacts.items():\n    status = \"âœ… FOUND\" if exists else \"âš ï¸  MISSING\"\n    print(f\"  {status}: {name}\")\n\n# Expected: At least 1 artifact should exist\n# If none exist: âš ï¸ Skipping (no tracking infrastructure)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3. Readiness Check #2: RAGAS Baseline Established\n\n**Requirement:** Identify divergence between automated scores and user satisfaction\n\n**Key Insight:**\n- RAGAS scores: 0.92 faithfulness, 0.88 relevance\n- User satisfaction: 82% â†’ 67% (DROP of 15 percentage points)\n- **Correlation threshold:** If RAGAS â†” satisfaction correlation > 0.90, human labeling may be unnecessary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #2: RAGAS Baseline & Satisfaction Divergence\nimport json\n\n# Expected: metrics.json or ragas_baseline.json with correlation data\nmetrics_file = Path(\"metrics.json\")\n\nif metrics_file.exists():\n    data = json.loads(metrics_file.read_text())\n    print(f\"RAGAS Faithfulness: {data.get('faithfulness', 'N/A')}\")\n    print(f\"User Satisfaction: {data.get('satisfaction', 'N/A')}\")\n    print(f\"Correlation: {data.get('correlation', 'N/A')}\")\nelse:\n    print(\"âš ï¸ Skipping (no metrics.json found)\")\n    print(\"# Expected: RAGAS > 0.85 but satisfaction < 0.75 indicates divergence\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Readiness Check #3: Budget Awareness\n\n**Requirement:** Understand cost implications of human annotation\n\n**Cost Estimates per 100 queries:**\n- ðŸ’° **Crowdsourced labeling:** $25-100\n- ðŸ’° **Expert annotators:** $250-750\n\n**Budget planning factors:**\n- Query volume (current: >100/week)\n- Annotation depth (quick ratings vs. detailed feedback)\n- Annotator expertise level",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #3: Budget Planning Calculator\nqueries_per_week = 100  # From requirement: >100 queries/week\nweeks_to_label = 4      # Initial labeling period\n\n# Cost estimates (per 100 queries)\ncrowdsourced_cost = (25, 100)  # min, max\nexpert_cost = (250, 750)\n\ntotal_queries = queries_per_week * weeks_to_label\nbatches = total_queries / 100\n\nprint(f\"ðŸ“Š Budget Estimate for {total_queries} queries ({weeks_to_label} weeks):\")\nprint(f\"  Crowdsourced: ${batches * crowdsourced_cost[0]:.0f} - ${batches * crowdsourced_cost[1]:.0f}\")\nprint(f\"  Expert:       ${batches * expert_cost[0]:.0f} - ${batches * expert_cost[1]:.0f}\")\nprint(\"\\n# Expected: Budget approval for chosen tier\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Readiness Check #4: Embrace Ambiguity\n\n**Requirement:** Accept that human annotation is subjective, not absolute\n\n**Key Mindset Shift:**\n- âŒ **OLD:** Automated tests = 100% reproducible (pass/fail)\n- âœ… **NEW:** Inter-annotator agreement typically 70-85%\n\n**What this means:**\n- Multiple annotators may disagree on the same query\n- Ambiguous cases require discussion and guidelines\n- Focus on patterns, not individual labels",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #4: Inter-Annotator Agreement Simulation\nimport random\nrandom.seed(42)\n\n# Simulate 3 annotators rating 10 queries (1-5 scale)\nnum_queries = 10\nannotators = [\"Annotator_A\", \"Annotator_B\", \"Annotator_C\"]\n\nprint(\"ðŸ“Š Sample Inter-Annotator Agreement:\")\nagreements = []\nfor q in range(3):  # Show first 3 queries\n    ratings = [random.randint(3, 5) for _ in annotators]\n    agreement = len(set(ratings)) == 1\n    agreements.append(agreement)\n    print(f\"  Query {q+1}: {ratings} - {'âœ… Agree' if agreement else 'âš ï¸  Disagree'}\")\n\nprint(f\"\\n# Expected: 70-85% agreement is normal (not 100%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Call-Forward: What M8.4 Will Introduce\n\n**M8.4 Focus:** Human-in-the-Loop Evaluation\n\n### Four Key Capabilities:\n\n#### 1. Feedback Collection\n- Detailed ratings: **Accuracy, Clarity, Helpfulness** (1-5 scales)\n- Goes beyond binary thumbs-up/down\n- Captures subjective dimensions automation misses\n\n#### 2. Active Learning\nPrioritize high-value queries:\n- **Blind spots** (high-RAGAS, low-satisfaction)\n- **Borderline RAGAS** scores\n- **New patterns** not in test sets\n- **High-stakes domains** (legal, medical)\n\n#### 3. Label Studio Integration\n- Structured annotation workflows\n- Multi-annotator support\n- Quality control mechanisms\n- Export to training datasets\n\n#### 4. Closed-Loop Improvement\nUse human labels to:\n- Refine prompts\n- Expand test sets\n- Retrain embeddings\n- Update evaluation criteria\n\n---\n\n### Critical Insight\n\n> **\"You can have 0.92 faithfulness and still ship unhelpful answersâ€”because automated evaluation doesn't capture user experience.\"**\n\n**When NOT to use human labeling:**\n- If RAGAS â†” satisfaction correlation > 0.90, automation is sufficient\n\n**PractaThon Exercise (30 min):**\nCreate `automation_gaps.md` documenting:\n- High-RAGAS/Low-satisfaction cases\n- Failure categories: Tone (35%), Structure (40%), Context (15%), Interpretation (10%)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create automation_gaps.md stub for PractaThon exercise\nstub_content = \"\"\"# Automation Gaps Analysis\n\n## High-RAGAS / Low-Satisfaction Cases\n\n### Case 1: [Query ID]\n- **RAGAS Score:** 0.92\n- **User Satisfaction:** 2/5\n- **Issue Category:** Tone (35%)\n- **Description:** [Technically correct but confusing delivery]\n\n### Case 2: [Query ID]\n- **RAGAS Score:** 0.89\n- **User Satisfaction:** 3/5\n- **Issue Category:** Structure (40%)\n- **Description:** [Verbose, lacking conciseness]\n\n## Failure Category Distribution\n- **Tone:** 35%\n- **Structure:** 40%\n- **Context:** 15%\n- **Interpretation:** 10%\n\n## Next Steps\n1. Annotate 100 queries using Label Studio\n2. Calculate inter-annotator agreement\n3. Update prompt templates based on findings\n\"\"\"\n\noutput_file = Path(\"automation_gaps.md\")\noutput_file.write_text(stub_content)\nprint(f\"âœ… Created: {output_file}\")\nprint(\"# Expected: Use this template for PractaThon exercise\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}