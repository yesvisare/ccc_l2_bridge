{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M6.4 → L3.M7.1 Readiness Validation\n",
    "## Security Complete → Observability Begins\n",
    "\n",
    "**Purpose**: Validate that M6.4 security deliverables are operational before starting M7.1 distributed tracing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: M6.4 Delivery Recap\n",
    "\n",
    "Module 6.4 completed the **enterprise security foundation** with four major achievements:\n",
    "\n",
    "### 1. Comprehensive Audit Trail\n",
    "- **ELK Stack Implementation**: Captures WHO (user_id, role, IP), WHAT (action, resource), WHEN (timestamp), and outcome\n",
    "- **Tamper-Proof Storage**: Hash chaining for audit integrity\n",
    "- **Centralized Aggregation**: Elasticsearch for compliance queries\n",
    "\n",
    "### 2. GDPR Automation\n",
    "- **Efficiency Gain**: 40 hours manual work → 5 minutes automated\n",
    "- **Right-to-Erasure**: Automated across systems with audit proof\n",
    "- **Consent Tracking**: Linked to processing actions\n",
    "\n",
    "### 3. Retention Policies\n",
    "- **Tiered Storage**:\n",
    "  - Hot: 0-90 days (Elasticsearch)\n",
    "  - Warm: 90 days-1 year (S3)\n",
    "  - Cold: 1-7 years (Glacier)\n",
    "- **Automatic Deletion**: After retention periods\n",
    "\n",
    "### 4. Complete Security Stack\n",
    "- M6.1: PII redaction\n",
    "- M6.2: Secrets management\n",
    "- M6.3: RBAC\n",
    "- M6.4: Compliance auditing\n",
    "\n",
    "**Result**: Enterprise-grade security posture achieved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Readiness Check #1 - ELK Stack Operational\n\n**Requirement**: Kibana accessible at localhost:5601 with audit-logs-* index showing events from last 24 hours; Elasticsearch query returns ≥100 events.\n\n**Pass Criteria**:\n- Kibana UI responds at http://localhost:5601\n- Index `audit-logs-*` exists\n- Event count ≥ 100 in last 24 hours",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import requests\nfrom datetime import datetime, timedelta\n\ndef check_elk_stack():\n    # Expected: Kibana responds, audit-logs-* index exists, ≥100 events in 24h\n    try:\n        # Check Kibana accessibility\n        kibana_url = \"http://localhost:5601/api/status\"\n        resp = requests.get(kibana_url, timeout=5)\n        print(f\"✓ Kibana: {resp.status_code}\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (Kibana not available): {e}\")\n        return\n    \n    # Check Elasticsearch index and event count\n    try:\n        es_url = \"http://localhost:9200/audit-logs-*/_count\"\n        query = {\"query\": {\"range\": {\"@timestamp\": {\"gte\": \"now-24h\"}}}}\n        resp = requests.post(es_url, json=query, timeout=5)\n        count = resp.json().get('count', 0)\n        status = \"✓\" if count >= 100 else \"✗\"\n        print(f\"{status} Events (24h): {count} (required: ≥100)\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (Elasticsearch not available): {e}\")\n\n# check_elk_stack()  # Uncomment to run",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Readiness Check #2 - Structured Logging\n\n**Requirement**: Application logs include unique `request_id` fields; correlation works between logs and audit events via request_id queries in Kibana.\n\n**Pass Criteria**:\n- Application logs contain `request_id` field\n- Correlation between logs and audit events via `request_id`\n- Kibana query can trace a request across both log types",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def check_structured_logging():\n    # Expected: Logs contain request_id, correlation works across log types\n    try:\n        es_url = \"http://localhost:9200/_search\"\n        query = {\n            \"size\": 1,\n            \"query\": {\"exists\": {\"field\": \"request_id\"}},\n            \"_source\": [\"request_id\", \"@timestamp\", \"message\"]\n        }\n        resp = requests.post(es_url, json=query, timeout=5)\n        hits = resp.json().get('hits', {}).get('hits', [])\n        \n        if hits:\n            req_id = hits[0]['_source'].get('request_id')\n            print(f\"✓ request_id found: {req_id[:16]}...\")\n            # Expected: Can query both logs and audit events with this request_id\n            print(f\"✓ Correlation: Query Kibana with 'request_id:\\\"{req_id}\\\"'\")\n        else:\n            print(\"✗ No logs with request_id field found\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (Elasticsearch not available): {e}\")\n\n# check_structured_logging()  # Uncomment to run",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Readiness Check #3 - Prometheus Metrics\n\n**Requirement**: Grafana dashboard at localhost:3000 updates every 15 seconds showing P50 (300-600ms baseline), P95 (700-1,200ms), request rates, and error rates <1%.\n\n**Pass Criteria**:\n- Grafana accessible at http://localhost:3000\n- Dashboard shows P50 latency: 300-600ms\n- Dashboard shows P95 latency: 700-1,200ms\n- Error rate < 1%",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def check_prometheus_metrics():\n    # Expected: Grafana accessible, P50: 300-600ms, P95: 700-1200ms, errors <1%\n    try:\n        grafana_url = \"http://localhost:3000/api/health\"\n        resp = requests.get(grafana_url, timeout=5)\n        print(f\"✓ Grafana: {resp.status_code}\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (Grafana not available): {e}\")\n        return\n    \n    # Expected: Query Prometheus for latency metrics\n    try:\n        prom_url = \"http://localhost:9090/api/v1/query\"\n        # Check P50 and P95 latencies (values in ms)\n        print(\"✓ Expected metrics: P50=300-600ms, P95=700-1200ms, errors<1%\")\n        print(\"  (Run: histogram_quantile(0.50, rate(http_request_duration_ms_bucket[5m])))\")\n        print(\"  (Run: histogram_quantile(0.95, rate(http_request_duration_ms_bucket[5m])))\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (Prometheus not available): {e}\")\n\n# check_prometheus_metrics()  # Uncomment to run",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Readiness Check #4 - M6 Completion\n\n**Requirement**: GitHub repository contains commits for M6.1-M6.4; each module's functionality verified.\n\n**Pass Criteria**:\n- M6.1: PII detection functional\n- M6.2: Vault secrets management operational\n- M6.3: RBAC enforcement working\n- M6.4: Elasticsearch audit events present",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport os\n\ndef check_m6_completion():\n    # Expected: Git commits for M6.1-M6.4, functionality verified\n    if not os.path.exists('.git'):\n        print(\"⚠️ Skipping (not a git repository)\")\n        return\n    \n    try:\n        # Check for M6-related commits\n        result = subprocess.run(['git', 'log', '--oneline', '--all', '--grep=M6'], \n                                capture_output=True, text=True, timeout=5)\n        commits = result.stdout.strip().split('\\n') if result.stdout.strip() else []\n        print(f\"✓ M6 commits found: {len(commits)}\")\n        \n        # Expected: Verify module functionality (stubs)\n        modules = ['M6.1 (PII)', 'M6.2 (Vault)', 'M6.3 (RBAC)', 'M6.4 (Audit)']\n        for module in modules:\n            print(f\"  - {module}: Manual verification required\")\n    except Exception as e:\n        print(f\"⚠️ Skipping (git check failed): {e}\")\n\n# check_m6_completion()  # Uncomment to run",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 6: Call-Forward to M7.1 - Distributed Tracing\n\n### The Gap We're About to Fill\n\n**Current State**: Prometheus metrics show aggregate P95 latency of 850ms, but provide **zero visibility** into why individual requests exceed this baseline.\n\n**The Problem**: A mysterious 4.2-second query cannot be diagnosed without request-level tracing.\n\n### What M7.1 Will Deliver\n\n**OpenTelemetry Instrumentation** enabling:\n\n1. **Request-Level Timing** (millisecond precision)\n   - Trace requests through retrieval → reranking → generation stages\n   - Identify exact bottlenecks in the pipeline\n\n2. **Service Dependency Visualization**\n   - See how failures cascade through the system\n   - Understand component interactions\n\n3. **Production Optimization Evidence**\n   - Example: Discover OpenAI calls consume 85% of latency\n   - Enable data-driven model selection decisions\n\n### Three Core Capabilities\n\n| Capability | Value |\n|------------|-------|\n| **Per-Request Breakdown** | Identify which operation causes slowness in each request |\n| **Cascading Failure Detection** | Visualize how one slow component impacts entire pipeline |\n| **Jaeger UI** | Interactive trace visualization and analysis |\n\n### Module 7 Roadmap (155 minutes total)\n\n- **M7.1** (42 min): OpenTelemetry instrumentation - \"Why was THIS query slow?\"\n- **M7.2** (38 min): Unified observability - Link metrics → logs → traces\n- **M7.3** (35 min): Performance profiling - CPU/memory hotspots\n- **M7.4** (40 min): Trace-based SLI monitoring and anomaly detection\n\n**Next Step**: Once all readiness checks pass, begin M7.1 to gain request-level visibility into system performance.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}