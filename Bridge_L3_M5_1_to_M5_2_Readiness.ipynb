{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Bridge L3.M5.1 → L3.M5.2 — Incremental Indexing to Data Pipelines\n\n**Track:** CCC Level 2 - Module 5: Production Data Management  \n**Bridge Type:** Within-Module  \n**Duration:** 8-10 minutes\n\n---\n\n## Purpose\n\nThis bridge validates that your **incremental indexing foundation from M5.1 is solid** before layering on orchestration complexity in M5.2. You've built a system that surgically updates only changed documents (95% faster, $49.90 cheaper per update). Now the question shifts: **Who runs it at 2 AM when documents update overnight?**\n\n**What shifts:** From manual incremental updates → automated data pipelines with Airflow  \n**Why it matters:** Without automation, you're burning $9,000/year in manual labor and risking stale data that damages user trust.\n\n---\n\n## Concepts Covered (Delta Only)\n\nThis bridge focuses on **readiness validation**, not new concepts:\n\n- **Checksum-based change detection** (verify it uses SHA-256, not timestamps)\n- **Persistent version metadata** (ensures data survives across runs)\n- **Targeted update logs** (proves incremental logic works before adding Airflow)\n\n**What's new:** The *cost framing* of manual processes—180 hours/year wasted on manual triggers.\n\n---\n\n## After Completing This Bridge\n\nYou will be able to:\n\n- ✓ Verify your incremental indexing script exists and runs correctly\n- ✓ Confirm SHA-256 checksums detect document changes (not unreliable timestamps)\n- ✓ Validate version history persists in `checksums.json` across script runs\n- ✓ Prove targeted updates work (logs show partial updates, not full re-indexing)\n- ✓ Quantify the $9K/year waste from manual triggers to justify M5.2 automation\n\n---\n\n## Context in Track\n\n**Previous:** [M5.1 Augmented - Incremental Indexing & Updates](../M5.1) (you built change detection + version tracking)  \n**Current:** Bridge validation—verify M5.1 foundation is solid  \n**Next:** [M5.2 Concept - Data Pipelines & Orchestration](../M5.2) (add Airflow automation + parallel processing)\n\n**Module Arc:** M5.1 (incremental updates) → **M5.2 (automation)** → M5.3 (monitoring) → M5.4 (data quality)\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "5l3xtcx46sp",
   "source": "## Run Locally\n\n**Windows (PowerShell):**\n```powershell\n$env:PYTHONPATH=\"$PWD\"; jupyter notebook Bridge_L3_M5_1_to_M5_2_Readiness.ipynb\n```\n\n**macOS/Linux:**\n```bash\nPYTHONPATH=$PWD jupyter notebook Bridge_L3_M5_1_to_M5_2_Readiness.ipynb\n```\n\n**All platforms:** This notebook runs offline—external service calls are gracefully skipped if resources are unavailable.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1) RECAP — What M5.1 Actually Shipped\n",
    "\n",
    "In **M5.1: Incremental Indexing & Updates**, you accomplished:\n",
    "\n",
    "✓ **Change detection system** — Uses SHA-256 checksums to detect document modifications in under 2 seconds (tested on 10,000 documents)\n",
    "\n",
    "✓ **Targeted Pinecone updates** — Deletes only modified chunks and inserts new versions, reducing update time from 20 minutes to 3-5 seconds (95% faster)\n",
    "\n",
    "✓ **Version tracking with rollback** — Maintains history of last 5 versions per document, enabling instant rollback when updates break production\n",
    "\n",
    "✓ **Atomic two-phase commit** — Ensures index never enters inconsistent state, even if process crashes mid-update\n",
    "\n",
    "**Key Achievement:** Transformed a system requiring $50 and 20 minutes for every update into one costing $0.10 and taking 3-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "id": "i31n1wihwgc",
   "source": "import os\nimport json\nfrom pathlib import Path\n\n# Check for incremental update script\nscript_exists = os.path.exists(\"incremental_update.py\")\nprint(f\"✓ Incremental update script exists: {script_exists}\")\n\n# Expected: \n# ✓ Incremental update script exists: True\n# ⚠️ If False: Script not found (implement in M5.1)\n\nif not script_exists:\n    print(\"⚠️ Skipping (no incremental_update.py found)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2lebmtdz2ib",
   "source": "import hashlib\n\n# Simulate checksum calculation\ndef calculate_checksum(content):\n    return hashlib.sha256(content.encode()).hexdigest()\n\n# Test change detection\ndoc_v1 = \"Original policy content\"\ndoc_v2 = \"Updated policy content\"\n\nchecksum_v1 = calculate_checksum(doc_v1)\nchecksum_v2 = calculate_checksum(doc_v2)\n\nprint(f\"✓ SHA-256 checksum calculation working\")\nprint(f\"  V1 checksum: {checksum_v1[:16]}...\")\nprint(f\"  V2 checksum: {checksum_v2[:16]}...\")\nprint(f\"  Changed: {checksum_v1 != checksum_v2}\")\n\n# Expected:\n# ✓ SHA-256 checksum calculation working\n# Changed: True",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h5j77kwioeo",
   "source": "# Check for persistent version tracking metadata\nchecksums_file = \"checksums.json\"\nchecksums_exists = os.path.exists(checksums_file)\n\nprint(f\"✓ Checksums metadata file exists: {checksums_exists}\")\n\nif checksums_exists:\n    with open(checksums_file, 'r') as f:\n        metadata = json.load(f)\n    print(f\"  Documents tracked: {len(metadata)}\")\n    print(f\"  Sample: {list(metadata.keys())[:2]}\")\nelse:\n    print(\"⚠️ Skipping (no checksums.json found)\")\n    # Create sample structure for reference\n    sample_metadata = {\n        \"policy_2024.pdf\": {\n            \"current_checksum\": \"abc123...\",\n            \"version_history\": [\"v1_hash\", \"v2_hash\", \"v3_hash\"]\n        }\n    }\n    print(f\"  Expected structure: {json.dumps(sample_metadata, indent=2)[:80]}...\")\n\n# Expected:\n# ✓ Checksums metadata file exists: True\n# Documents tracked: 50",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nt1tfz53kwh",
   "source": "# Check for update logs showing incremental behavior\nlog_file = \"update_log.txt\"\nlog_exists = os.path.exists(log_file)\n\nprint(f\"✓ Update log exists: {log_exists}\")\n\nif log_exists:\n    with open(log_file, 'r') as f:\n        lines = f.readlines()[:5]  # Read first 5 lines\n    print(f\"  Log entries: {len(lines)}\")\n    for line in lines:\n        print(f\"    {line.strip()}\")\nelse:\n    print(\"⚠️ Skipping (no update_log.txt found)\")\n    print(\"\\n  Expected format:\")\n    print(\"  [2025-11-02 02:00:01] Starting incremental update\")\n    print(\"  [2025-11-02 02:00:03] Detected 3 changed documents\")\n    print(\"  [2025-11-02 02:00:18] Processed policy_2024.pdf (5 chunks)\")\n    print(\"  [2025-11-02 02:00:24] Update complete: 3 docs, 21 seconds\")\n\n# Expected:\n# ✓ Update log exists: True\n# Shows targeted updates (not full re-index)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3dhr5vkq99s",
   "source": "## 6) CALL-FORWARD — What M5.2 Will Introduce and Why\\n\\n### The Problem: Who Runs It at 2 AM?\\n\\nYour incremental indexing works perfectly—detects changes instantly, updates surgically, tracks versions. But it's still a **manual process**.\\n\\n**Current state:** You run `python incremental_update.py` manually\\n\\n**The burn rate:**\\n- **Time cost:** 30 minutes per day checking for updates = 180 hours per year\\n- **Opportunity cost:** While running updates, you're not building features = $9,000 lost productivity (@$50/hour)\\n- **Risk cost:** Miss one update = compliance violation = legal exposure\\n\\n**Total hidden cost:** $9,000+ per year in manual labor\\n\\n---\\n\\n### What M5.2: Data Pipelines & Orchestration Will Add\\n\\n**1. Automated scheduling with Apache Airflow**\\n   → No more manual triggers—pipelines run daily, hourly, or on-demand with zero human intervention\\n\\n**2. Parallel processing that cuts time from 40 minutes to 8 minutes**\\n   → For 5,000 documents, process 4-8 at a time instead of sequentially\\n\\n**3. Graceful error handling with automatic retries and alerting**\\n   → One failed document doesn't crash entire pipeline, Slack alerts on failures\\n\\n---\\n\\n### The Bridge Question\\n\\n**\\\"Your incremental indexing works perfectly—but who runs it at 2 AM when documents update overnight?\\\"**\\n\\nM5.2 will answer this by automating your data refresh pipeline.\\n\\n---\\n\\n**Next:** [M5.2 Concept - Data Pipelines & Orchestration](https://github.com/yesvisare/ccc_l2_bridge)\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "m8gv5ob86m8",
   "source": "## 5) Readiness Check #4 — Successfully Updated at Least One Document Without Full Re-Index\n\n**Check:** Update log shows targeted update (not full corpus reprocessing)  \n**Impact:** Confirms incremental logic works before adding orchestration layer\n\n**What to verify:**\n- Update log exists showing partial updates\n- Only changed documents were processed (not all documents)\n- Update time < 10 seconds (not minutes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "akmedvxlz2d",
   "source": "**What this cell does:** Reads the first 5 lines of `update_log.txt` to verify targeted incremental updates occurred (not full re-indexing). Displays expected log format if file is missing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "acm1gcl1scj",
   "source": "## 4) Readiness Check #3 — Version Tracking Metadata Stored Persistently\n\n**Check:** `checksums.json` file exists with version history for all documents  \n**Impact:** Prevents data loss during Airflow migration (metadata must persist across runs)\n\n**What to verify:**\n- Checksum metadata file exists\n- Contains version history (last 5 versions per document)\n- Persists across script runs (not in-memory only)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7bjna731zo7",
   "source": "**What this cell does:** Attempts to load `checksums.json` to verify persistent version tracking metadata exists. If missing, shows expected structure and continues gracefully.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ku54998drv",
   "source": "## 3) Readiness Check #2 — Change Detection Using Checksums\n\n**Check:** Modify one document, verify checksum changes and triggers update  \n**Impact:** Saves 4+ hours debugging why \"automation isn't detecting changes\"\n\n**What to verify:**\n- Checksum calculation uses SHA-256 (not timestamps)\n- Modified document produces different checksum\n- Change detection triggers correctly",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "elcyqiyjxxd",
   "source": "**What this cell does:** Simulates SHA-256 checksum calculation on two document versions to demonstrate that content changes produce different hashes, proving change detection works.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "5h6a7cor9rh",
   "source": "## 2) Readiness Check #1 — Incremental Indexing Implemented and Tested\n\n**Check:** Run incremental update on test corpus, verify only changed docs process  \n**Impact:** Saves 2 hours debugging Airflow if base pipeline is broken\n\n**What to verify:**\n- Incremental update script exists and runs successfully\n- Only modified documents are processed (not full re-index)\n- Update completes in reasonable time (seconds, not minutes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2o5fmoa78j3",
   "source": "**What this cell does:** Checks if `incremental_update.py` exists in the current directory. If missing, prints a skip warning so the notebook runs offline without errors.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}