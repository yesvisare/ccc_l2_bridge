{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Bridge L3.M5.1 → L3.M5.2 — Incremental Indexing to Data Pipelines\n",
    "\n",
    "**Track:** CCC Level 2 - Module 5: Production Data Management  \n",
    "**Purpose:** Validation notebook for bridge readiness checks  \n",
    "**Duration:** 8-10 minutes  \n",
    "**Bridge Type:** Within-Module  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1) RECAP — What M5.1 Actually Shipped\n",
    "\n",
    "In **M5.1: Incremental Indexing & Updates**, you accomplished:\n",
    "\n",
    "✓ **Change detection system** — Uses SHA-256 checksums to detect document modifications in under 2 seconds (tested on 10,000 documents)\n",
    "\n",
    "✓ **Targeted Pinecone updates** — Deletes only modified chunks and inserts new versions, reducing update time from 20 minutes to 3-5 seconds (95% faster)\n",
    "\n",
    "✓ **Version tracking with rollback** — Maintains history of last 5 versions per document, enabling instant rollback when updates break production\n",
    "\n",
    "✓ **Atomic two-phase commit** — Ensures index never enters inconsistent state, even if process crashes mid-update\n",
    "\n",
    "**Key Achievement:** Transformed a system requiring $50 and 20 minutes for every update into one costing $0.10 and taking 3-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "id": "i31n1wihwgc",
   "source": "import os\nimport json\nfrom pathlib import Path\n\n# Check for incremental update script\nscript_exists = os.path.exists(\"incremental_update.py\")\nprint(f\"✓ Incremental update script exists: {script_exists}\")\n\n# Expected: \n# ✓ Incremental update script exists: True\n# ⚠️ If False: Script not found (implement in M5.1)\n\nif not script_exists:\n    print(\"⚠️ Skipping (no incremental_update.py found)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2lebmtdz2ib",
   "source": "import hashlib\n\n# Simulate checksum calculation\ndef calculate_checksum(content):\n    return hashlib.sha256(content.encode()).hexdigest()\n\n# Test change detection\ndoc_v1 = \"Original policy content\"\ndoc_v2 = \"Updated policy content\"\n\nchecksum_v1 = calculate_checksum(doc_v1)\nchecksum_v2 = calculate_checksum(doc_v2)\n\nprint(f\"✓ SHA-256 checksum calculation working\")\nprint(f\"  V1 checksum: {checksum_v1[:16]}...\")\nprint(f\"  V2 checksum: {checksum_v2[:16]}...\")\nprint(f\"  Changed: {checksum_v1 != checksum_v2}\")\n\n# Expected:\n# ✓ SHA-256 checksum calculation working\n# Changed: True",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h5j77kwioeo",
   "source": "# Check for persistent version tracking metadata\nchecksums_file = \"checksums.json\"\nchecksums_exists = os.path.exists(checksums_file)\n\nprint(f\"✓ Checksums metadata file exists: {checksums_exists}\")\n\nif checksums_exists:\n    with open(checksums_file, 'r') as f:\n        metadata = json.load(f)\n    print(f\"  Documents tracked: {len(metadata)}\")\n    print(f\"  Sample: {list(metadata.keys())[:2]}\")\nelse:\n    print(\"⚠️ Skipping (no checksums.json found)\")\n    # Create sample structure for reference\n    sample_metadata = {\n        \"policy_2024.pdf\": {\n            \"current_checksum\": \"abc123...\",\n            \"version_history\": [\"v1_hash\", \"v2_hash\", \"v3_hash\"]\n        }\n    }\n    print(f\"  Expected structure: {json.dumps(sample_metadata, indent=2)[:80]}...\")\n\n# Expected:\n# ✓ Checksums metadata file exists: True\n# Documents tracked: 50",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nt1tfz53kwh",
   "source": "# Check for update logs showing incremental behavior\nlog_file = \"update_log.txt\"\nlog_exists = os.path.exists(log_file)\n\nprint(f\"✓ Update log exists: {log_exists}\")\n\nif log_exists:\n    with open(log_file, 'r') as f:\n        lines = f.readlines()[:5]  # Read first 5 lines\n    print(f\"  Log entries: {len(lines)}\")\n    for line in lines:\n        print(f\"    {line.strip()}\")\nelse:\n    print(\"⚠️ Skipping (no update_log.txt found)\")\n    print(\"\\n  Expected format:\")\n    print(\"  [2025-11-02 02:00:01] Starting incremental update\")\n    print(\"  [2025-11-02 02:00:03] Detected 3 changed documents\")\n    print(\"  [2025-11-02 02:00:18] Processed policy_2024.pdf (5 chunks)\")\n    print(\"  [2025-11-02 02:00:24] Update complete: 3 docs, 21 seconds\")\n\n# Expected:\n# ✓ Update log exists: True\n# Shows targeted updates (not full re-index)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3dhr5vkq99s",
   "source": "## 6) CALL-FORWARD — What M5.2 Will Introduce and Why\\n\\n### The Problem: Who Runs It at 2 AM?\\n\\nYour incremental indexing works perfectly—detects changes instantly, updates surgically, tracks versions. But it's still a **manual process**.\\n\\n**Current state:** You run `python incremental_update.py` manually\\n\\n**The burn rate:**\\n- **Time cost:** 30 minutes per day checking for updates = 180 hours per year\\n- **Opportunity cost:** While running updates, you're not building features = $9,000 lost productivity (@$50/hour)\\n- **Risk cost:** Miss one update = compliance violation = legal exposure\\n\\n**Total hidden cost:** $9,000+ per year in manual labor\\n\\n---\\n\\n### What M5.2: Data Pipelines & Orchestration Will Add\\n\\n**1. Automated scheduling with Apache Airflow**\\n   → No more manual triggers—pipelines run daily, hourly, or on-demand with zero human intervention\\n\\n**2. Parallel processing that cuts time from 40 minutes to 8 minutes**\\n   → For 5,000 documents, process 4-8 at a time instead of sequentially\\n\\n**3. Graceful error handling with automatic retries and alerting**\\n   → One failed document doesn't crash entire pipeline, Slack alerts on failures\\n\\n---\\n\\n### The Bridge Question\\n\\n**\\\"Your incremental indexing works perfectly—but who runs it at 2 AM when documents update overnight?\\\"**\\n\\nM5.2 will answer this by automating your data refresh pipeline.\\n\\n---\\n\\n**Next:** [M5.2 Concept - Data Pipelines & Orchestration](https://github.com/yesvisare/ccc_l2_bridge)\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "m8gv5ob86m8",
   "source": "## 5) Readiness Check #4 — Successfully Updated at Least One Document Without Full Re-Index\n\n**Check:** Update log shows targeted update (not full corpus reprocessing)  \n**Impact:** Confirms incremental logic works before adding orchestration layer\n\n**What to verify:**\n- Update log exists showing partial updates\n- Only changed documents were processed (not all documents)\n- Update time < 10 seconds (not minutes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "acm1gcl1scj",
   "source": "## 4) Readiness Check #3 — Version Tracking Metadata Stored Persistently\n\n**Check:** `checksums.json` file exists with version history for all documents  \n**Impact:** Prevents data loss during Airflow migration (metadata must persist across runs)\n\n**What to verify:**\n- Checksum metadata file exists\n- Contains version history (last 5 versions per document)\n- Persists across script runs (not in-memory only)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ku54998drv",
   "source": "## 3) Readiness Check #2 — Change Detection Using Checksums\n\n**Check:** Modify one document, verify checksum changes and triggers update  \n**Impact:** Saves 4+ hours debugging why \"automation isn't detecting changes\"\n\n**What to verify:**\n- Checksum calculation uses SHA-256 (not timestamps)\n- Modified document produces different checksum\n- Change detection triggers correctly",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "5h6a7cor9rh",
   "source": "## 2) Readiness Check #1 — Incremental Indexing Implemented and Tested\n\n**Check:** Run incremental update on test corpus, verify only changed docs process  \n**Impact:** Saves 2 hours debugging Airflow if base pipeline is broken\n\n**What to verify:**\n- Incremental update script exists and runs successfully\n- Only modified documents are processed (not full re-index)\n- Update completes in reasonable time (seconds, not minutes)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}