{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M7.2 → L3.M7.3 Readiness Validation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This bridge validates that **M7.2 (APM Complete)** successfully delivered production-grade observability before advancing to **M7.3 (Business Metrics)**. M7.2 focused on technical performance monitoring—flame graphs, memory profiling, query optimization. M7.3 shifts to **business-level questions**: Which features drive retention? What's the cost-per-user? How do we detect satisfaction drops before churn?\n",
    "\n",
    "Without validated APM infrastructure and cost tracking from M7.2, M7.3's cohort analysis would lack the foundational data layer needed to answer executive questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts Covered\n",
    "\n",
    "**Delta from M7.2:**\n",
    "- Validating APM infrastructure is live (not just installed)\n",
    "- Confirming user feedback collection exists (prerequisite for satisfaction metrics)\n",
    "- Verifying cost tracking foundations (OpenAI, Pinecone)\n",
    "\n",
    "**New for M7.3 (call-forward):**\n",
    "- RAG-specific quality metrics (accuracy, satisfaction scoring)\n",
    "- Cohort segmentation (enterprise vs. free tier, query categories)\n",
    "- Business KPI translation (revenue-per-query, cost-per-active-user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Completing This Notebook\n",
    "\n",
    "You will be able to:\n",
    "- Verify Datadog APM is actively profiling your application (flame graphs visible)\n",
    "- Confirm Prometheus/Grafana dashboards are receiving live metrics\n",
    "- Validate user feedback mechanisms are capturing ratings for analysis\n",
    "- Demonstrate cost tracking for LLM and vector database services\n",
    "- Articulate the business questions M7.3 will address (satisfaction drop investigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context in Track\n",
    "\n",
    "**Bridge:** L3.M7.2 → L3.M7.3  \n",
    "**Prerequisite:** Module 7.2 (APM Complete)  \n",
    "**Next Module:** Module 7.3 (Custom Business Metrics)  \n",
    "\n",
    "**Run Locally (Windows):**\n",
    "```powershell\n",
    "powershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook\"\n",
    "```\n",
    "\n",
    "**Run Locally (macOS/Linux):**\n",
    "```bash\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Recap — What M7.2 Shipped\n",
    "\n",
    "Module 7.2 (APM Complete) delivered comprehensive application performance monitoring:\n",
    "\n",
    "### 1. Datadog APM Integration\n",
    "- Installed `ddtrace` with API keys configured\n",
    "- Continuous profiling at 5% sampling\n",
    "- Maintained <3% CPU overhead\n",
    "\n",
    "### 2. Function-Level Bottleneck Analysis\n",
    "- Flame graphs identifying specific performance issues\n",
    "- Discovered `serialize_prompt()` requiring 750ms optimization\n",
    "- Found O(n²) loop at line 187\n",
    "\n",
    "### 3. Memory Profiling\n",
    "- Heap growth tracking enabled\n",
    "- Identified unbounded `document_cache` dictionary\n",
    "- Implemented LRU eviction policies\n",
    "\n",
    "### 4. Database Query Optimization\n",
    "- APM visualization of SQL queries\n",
    "- Exposed N+1 query problems\n",
    "- Optimized to single JOIN queries → 10x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Readiness Check #1 — Datadog Flame Graphs\n",
    "\n",
    "**Requirement:** Datadog UI must display flame graphs from the past hour.\n",
    "\n",
    "**What to verify:**\n",
    "- Datadog APM is actively receiving traces\n",
    "- Flame graph visualization is available\n",
    "- Recent profiling data (last 60 minutes) exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Datadog API Keys\n",
    "\n",
    "This cell verifies Datadog credentials are configured. If keys are missing, the check gracefully skips (acceptable for offline/demo environments). With keys present, manual verification in the Datadog UI is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check 1: Datadog flame graphs availability\n",
    "dd_api_key = os.getenv(\"DD_API_KEY\")\n",
    "dd_app_key = os.getenv(\"DD_APP_KEY\")\n",
    "\n",
    "if not dd_api_key or not dd_app_key:\n",
    "    print(\"⚠️  Skipping (no Datadog keys)\")\n",
    "    print(\"   To enable: Set DD_API_KEY and DD_APP_KEY environment variables\")\n",
    "else:\n",
    "    print(\"✓ Datadog keys configured\")\n",
    "    print(\"  Manual step: Open Datadog UI → APM → Profiling → Flame Graphs (last 1h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Readiness Check #2 — Grafana/Prometheus Metrics\n",
    "\n",
    "**Requirement:** Grafana must confirm Prometheus metrics are actively updating.\n",
    "\n",
    "**What to verify:**\n",
    "- Prometheus is scraping application metrics\n",
    "- Grafana dashboards show recent data\n",
    "- Key metrics (request_count, latency_p95, error_rate) are updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Prometheus and Grafana Endpoints\n",
    "\n",
    "This cell checks if Prometheus and Grafana service URLs are configured. No actual HTTP requests are made (offline-safe). Manual dashboard verification is required when services are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Prometheus/Grafana metrics availability\n",
    "prometheus_url = os.getenv(\"PROMETHEUS_URL\", \"http://localhost:9090\")\n",
    "grafana_url = os.getenv(\"GRAFANA_URL\", \"http://localhost:3000\")\n",
    "\n",
    "print(f\"Prometheus endpoint: {prometheus_url}\")\n",
    "print(f\"Grafana endpoint: {grafana_url}\")\n",
    "print(\"⚠️  Manual verification required:\")\n",
    "print(\"   Open Grafana dashboard, verify metrics updated in last 5 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Readiness Check #3 — User Feedback Mechanism\n",
    "\n",
    "**Requirement:** User feedback capability (thumbs up/down) must exist in the RAG API.\n",
    "\n",
    "**What to verify:**\n",
    "- API endpoint accepts feedback (rating/thumbs)\n",
    "- Feedback is stored for analysis\n",
    "- Basic schema: query_id, rating, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Expected Feedback Schema\n",
    "\n",
    "This cell defines the minimal schema required for user feedback collection. The schema serves as a contract for M7.3's satisfaction analysis. No external calls are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: User feedback mechanism stub\n",
    "feedback_schema = {\n",
    "    \"query_id\": \"str\",\n",
    "    \"rating\": \"int (1=down, 5=up) or bool\",\n",
    "    \"timestamp\": \"datetime\"\n",
    "}\n",
    "\n",
    "print(\"✓ Expected feedback schema defined\")\n",
    "print(\"  Manual step: Verify POST /api/feedback endpoint exists\")\n",
    "print(\"  Expected behavior: Accepts {query_id, rating}, returns 200 OK\")\n",
    "print(\"  Storage: Database table 'user_feedback' contains recent entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Readiness Check #4 — Cost Data Documentation\n",
    "\n",
    "**Requirement:** Cost data must be documented for OpenAI and Pinecone services.\n",
    "\n",
    "**What to verify:**\n",
    "- Cost tracking exists for API usage\n",
    "- Recent billing data available (OpenAI tokens, Pinecone queries)\n",
    "- Documentation includes: service, metric, cost/unit, monthly estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cost Tracking Template\n",
    "\n",
    "This cell establishes the expected structure for cost documentation. M7.3 will use this data for cohort-level cost analysis (cost-per-user, revenue-per-query). Template values are placeholders; actual usage data should exist in your billing dashboard or costs.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 4: Cost data documentation stub\n",
    "cost_template = {\n",
    "    \"openai\": {\"tokens_used\": 0, \"cost_per_1k\": 0.002, \"monthly_est\": 0},\n",
    "    \"pinecone\": {\"queries\": 0, \"cost_per_1k\": 0.001, \"monthly_est\": 0}\n",
    "}\n",
    "\n",
    "print(\"✓ Cost tracking template defined\")\n",
    "print(\"  Manual step: Verify costs.json or billing dashboard exists\")\n",
    "print(\"  Expected: Contains actual usage data from last 30 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Call-Forward — What M7.3 Will Introduce\n",
    "\n",
    "Module 7.3 (Custom Business Metrics) builds on M7.2's APM foundation to answer critical business questions.\n",
    "\n",
    "### 1. RAG-Specific Quality Metrics\n",
    "- **Answer Accuracy Tracking**: Per-query correctness measurement\n",
    "- **Satisfaction Scores**: User rating aggregation and trending\n",
    "- **Automated Degradation Alerts**: Threshold-based notifications when quality drops\n",
    "\n",
    "### 2. Cohort Analysis\n",
    "- **Cost Segmentation by User Type**: Enterprise vs. free tier usage patterns\n",
    "- **Feature Usage Patterns**: Which RAG features drive the most engagement\n",
    "- **Retention by Query Category**: Identify high-value query types\n",
    "\n",
    "### 3. Executive Dashboards\n",
    "- **Business Language Translation**: Convert technical metrics to KPIs\n",
    "- **Revenue per Query**: Monetization efficiency tracking\n",
    "- **Cost per Active User**: Unit economics for sustainable growth\n",
    "\n",
    "### Driving Question for M7.3\n",
    "**Scenario**: Satisfaction score dropped from 4.2 → 3.8 in the past week.  \n",
    "**Investigation**: Which query types are problematic? Which user cohorts are affected? What's the cost impact?\n",
    "\n",
    "---\n",
    "\n",
    "## Pass Criteria\n",
    "\n",
    "All 4 readiness checks must pass (or show documented workarounds):\n",
    "1. ✓ Datadog flame graphs visible (last 1 hour)\n",
    "2. ✓ Grafana/Prometheus metrics updating (last 5 minutes)\n",
    "3. ✓ User feedback endpoint functional\n",
    "4. ✓ Cost data documented (OpenAI + Pinecone)\n",
    "\n",
    "**Next Step**: Proceed to Module 7.3 to implement custom business metrics and cohort analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
