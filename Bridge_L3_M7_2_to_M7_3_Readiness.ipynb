{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M7.2 → L3.M7.3 Readiness Validation\n",
    "\n",
    "**From:** APM Complete  \n",
    "**To:** Business Metrics  \n",
    "**Purpose:** Validate M7.2 deliverables before advancing to M7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap — What M7.2 Shipped\n",
    "\n",
    "Module 7.2 (APM Complete) delivered comprehensive application performance monitoring:\n",
    "\n",
    "### 1. Datadog APM Integration\n",
    "- Installed `ddtrace` with API keys configured\n",
    "- Continuous profiling at 5% sampling\n",
    "- Maintained <3% CPU overhead\n",
    "\n",
    "### 2. Function-Level Bottleneck Analysis\n",
    "- Flame graphs identifying specific performance issues\n",
    "- Discovered `serialize_prompt()` requiring 750ms optimization\n",
    "- Found O(n²) loop at line 187\n",
    "\n",
    "### 3. Memory Profiling\n",
    "- Heap growth tracking enabled\n",
    "- Identified unbounded `document_cache` dictionary\n",
    "- Implemented LRU eviction policies\n",
    "\n",
    "### 4. Database Query Optimization\n",
    "- APM visualization of SQL queries\n",
    "- Exposed N+1 query problems\n",
    "- Optimized to single JOIN queries → 10x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Readiness Check #1 — Datadog Flame Graphs\n\n**Requirement:** Datadog UI must display flame graphs from the past hour.\n\n**What to verify:**\n- Datadog APM is actively receiving traces\n- Flame graph visualization is available\n- Recent profiling data (last 60 minutes) exists",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Check 1: Datadog flame graphs availability\ndd_api_key = os.getenv(\"DD_API_KEY\")\ndd_app_key = os.getenv(\"DD_APP_KEY\")\n\nif not dd_api_key or not dd_app_key:\n    print(\"⚠️  Skipping (no Datadog keys)\")\nelse:\n    print(\"✓ Datadog keys configured\")\n    # Expected: Manual verification in Datadog UI\n    # Navigate to: APM → Profiling → Flame Graphs (last 1h)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Readiness Check #2 — Grafana/Prometheus Metrics\n\n**Requirement:** Grafana must confirm Prometheus metrics are actively updating.\n\n**What to verify:**\n- Prometheus is scraping application metrics\n- Grafana dashboards show recent data\n- Key metrics (request_count, latency_p95, error_rate) are updating",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check 2: Prometheus/Grafana metrics availability\nprometheus_url = os.getenv(\"PROMETHEUS_URL\", \"http://localhost:9090\")\ngrafana_url = os.getenv(\"GRAFANA_URL\", \"http://localhost:3000\")\n\ntry:\n    # Stub: Check if services are reachable\n    print(f\"Prometheus: {prometheus_url}\")\n    print(f\"Grafana: {grafana_url}\")\n    print(\"⚠️  Manual verification required\")\n    # Expected: Open Grafana dashboard, verify metrics updating in last 5m\nexcept Exception as e:\n    print(f\"⚠️  Skipping (services unavailable): {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Readiness Check #3 — User Feedback Mechanism\n\n**Requirement:** User feedback capability (thumbs up/down) must exist in the RAG API.\n\n**What to verify:**\n- API endpoint accepts feedback (rating/thumbs)\n- Feedback is stored for analysis\n- Basic schema: query_id, rating, timestamp",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check 3: User feedback mechanism stub\nfeedback_schema = {\n    \"query_id\": \"str\",\n    \"rating\": \"int (1=down, 5=up) or bool\",\n    \"timestamp\": \"datetime\"\n}\n\nprint(\"✓ Expected feedback schema defined\")\nprint(\"Verify: POST /api/feedback endpoint exists\")\n# Expected: Endpoint accepts {query_id, rating} and returns 200 OK\n# Database table 'user_feedback' contains recent entries",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Readiness Check #4 — Cost Data Documentation\n\n**Requirement:** Cost data must be documented for OpenAI and Pinecone services.\n\n**What to verify:**\n- Cost tracking exists for API usage\n- Recent billing data available (OpenAI tokens, Pinecone queries)\n- Documentation includes: service, metric, cost/unit, monthly estimate",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check 4: Cost data documentation stub\ncost_template = {\n    \"openai\": {\"tokens_used\": 0, \"cost_per_1k\": 0.002, \"monthly_est\": 0},\n    \"pinecone\": {\"queries\": 0, \"cost_per_1k\": 0.001, \"monthly_est\": 0}\n}\n\nprint(\"✓ Cost tracking template defined\")\nprint(\"Verify: costs.json or billing dashboard exists\")\n# Expected: File contains actual usage data from last 30 days",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Call-Forward — What M7.3 Will Introduce\n\nModule 7.3 (Custom Business Metrics) builds on M7.2's APM foundation to answer critical business questions.\n\n### 1. RAG-Specific Quality Metrics\n- **Answer Accuracy Tracking**: Per-query correctness measurement\n- **Satisfaction Scores**: User rating aggregation and trending\n- **Automated Degradation Alerts**: Threshold-based notifications when quality drops\n\n### 2. Cohort Analysis\n- **Cost Segmentation by User Type**: Enterprise vs. free tier usage patterns\n- **Feature Usage Patterns**: Which RAG features drive the most engagement\n- **Retention by Query Category**: Identify high-value query types\n\n### 3. Executive Dashboards\n- **Business Language Translation**: Convert technical metrics to KPIs\n- **Revenue per Query**: Monetization efficiency tracking\n- **Cost per Active User**: Unit economics for sustainable growth\n\n### Driving Question for M7.3\n**Scenario**: Satisfaction score dropped from 4.2 → 3.8 in the past week.  \n**Investigation**: Which query types are problematic? Which user cohorts are affected? What's the cost impact?\n\n---\n\n## Pass Criteria\n\nAll 4 readiness checks must pass (or show documented workarounds):\n1. ✓ Datadog flame graphs visible (last 1 hour)\n2. ✓ Grafana/Prometheus metrics updating (last 5 minutes)\n3. ✓ User feedback endpoint functional\n4. ✓ Cost data documented (OpenAI + Pinecone)\n\n**Next Step**: Proceed to Module 7.3 to implement custom business metrics and cohort analysis.\"",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}