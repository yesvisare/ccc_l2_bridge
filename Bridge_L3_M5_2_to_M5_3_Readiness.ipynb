{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge L3.M5.2 → L3.M5.3 Readiness Validation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This bridge validates the transition from **Data Pipelines** (M5.2 - production automation) to **Data Quality** (M5.3 - quality scoring and drift detection).\n",
    "\n",
    "**What Shifts:** You move from running pipelines reliably at scale to ensuring the data flowing through them meets quality standards. M5.2 gave you throughput and reliability; M5.3 adds visibility into what you're actually indexing.\n",
    "\n",
    "**Why It Matters:** A pipeline processing 5,000 documents in 8 minutes is worthless if those documents contain corrupted text, duplicates, or degraded data. Quality checks prevent garbage-in-garbage-out scenarios that undermine RAG system accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts Covered\n",
    "\n",
    "This bridge introduces **validation gates** as a prerequisite pattern:\n",
    "\n",
    "- **Infrastructure Readiness Checks** - Verifying production services are operational before layering new capabilities\n",
    "- **Minimum Viable Dataset** - Establishing thresholds for meaningful quality analysis\n",
    "- **Monitoring Foundation** - Ensuring metrics infrastructure exists to extend with quality signals\n",
    "\n",
    "**Delta from M5.2:** No new pipeline features; only validation that existing automation is production-ready for quality instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Completing This Bridge\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Verify Airflow DAGs have run successfully for 3+ consecutive days\n",
    "- Confirm parallel processing workers (Celery) are active and handling load\n",
    "- Validate Prometheus metrics are collecting and visible in Grafana\n",
    "- Check vector database contains minimum dataset size (1,000+ documents) for quality analysis\n",
    "- Understand why each readiness check blocks M5.3 progress if failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context in Track\n",
    "\n",
    "**Track:** Level 3 - Production RAG Systems\n",
    "\n",
    "**Bridge Position:** M5.2 → M5.3\n",
    "\n",
    "**Previous Module:** L3.M5.2 Data Pipelines (Airflow scheduling, Celery parallelization, error handling, Prometheus monitoring)\n",
    "\n",
    "**Next Module:** L3.M5.3 Data Quality (chunk quality scoring, duplicate detection, data drift monitoring)\n",
    "\n",
    "**Estimated Time:** 15-20 minutes (validation checks only; no new implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Locally\n",
    "\n",
    "**Windows (PowerShell):**\n",
    "```powershell\n",
    "powershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook\"\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "```bash\n",
    "PYTHONPATH=$PWD jupyter notebook\n",
    "```\n",
    "\n",
    "**Dependencies:** Python 3.8+, Jupyter Notebook\n",
    "\n",
    "**Optional Service SDKs:** `apache-airflow`, `celery`, `prometheus-client`, `pinecone-client` (only needed for automated checks; manual verification via dashboards works without these)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Recap: What M5.2 Delivered\n",
    "\n",
    "Module 5.2 (Data Pipelines) shipped four production-grade capabilities:\n",
    "\n",
    "### 1.1 Automated Scheduling\n",
    "- **Achievement:** DAGs run daily at 2 AM with zero manual intervention\n",
    "- **Technology:** Airflow scheduling\n",
    "\n",
    "### 1.2 Parallel Processing\n",
    "- **Achievement:** 5,000 documents process in 8 minutes instead of 40 (5x speedup)\n",
    "- **Technology:** 4-8 Celery workers\n",
    "\n",
    "### 1.3 Error Handling\n",
    "- **Achievement:** Single document failures don't crash pipeline\n",
    "- **Technology:** Automatic retries with exponential backoff\n",
    "\n",
    "### 1.4 Production Monitoring\n",
    "- **Achievement:** Real-time metrics tracking\n",
    "- **Metrics:** Success rate, P95 latency, error types\n",
    "- **Technology:** Prometheus + Grafana dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Readiness Check #1: Airflow DAGs Running Successfully\n",
    "\n",
    "**Requirement:** Verify green runs for last 3 days in Airflow UI\n",
    "\n",
    "**Why Critical:** Prevents 3+ hours debugging quality checks on broken pipelines. Quality instrumentation assumes stable base pipeline execution.\n",
    "\n",
    "**Pass Criteria:** All scheduled DAG runs show SUCCESS status for 72+ hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this code does:** Checks for `AIRFLOW_HOME` environment variable to confirm Airflow is configured. Skips gracefully if not present (offline-friendly), allowing manual dashboard verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Check if Airflow is configured\n",
    "AIRFLOW_HOME = os.getenv('AIRFLOW_HOME')\n",
    "\n",
    "if not AIRFLOW_HOME:\n",
    "    print(\"⚠️ Skipping (no Airflow configured)\")\n",
    "    print(\"To validate: Check Airflow UI for DAG runs in last 3 days\")\n",
    "else:\n",
    "    # Expected: Query DAG runs via Airflow API\n",
    "    # Expected: dag_runs = [{\"state\": \"success\", \"date\": \"2025-11-05\"}, ...]\n",
    "    # Expected: all([r[\"state\"] == \"success\" for r in dag_runs]) == True\n",
    "    print(f\"✓ Airflow home: {AIRFLOW_HOME}\")\n",
    "    print(\"Manual verification required: Check Airflow UI for green runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Readiness Check #2: Parallel Processing Active (4-8 Workers)\n",
    "\n",
    "**Requirement:** Verify Flower dashboard shows all workers active during runs\n",
    "\n",
    "**Why Critical:** Quality checks add 20% overhead to processing time. Without parallelization, your 8-minute pipeline becomes 10+ minutes, degrading throughput.\n",
    "\n",
    "**Pass Criteria:** 4-8 Celery workers registered and actively processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this code does:** Detects Celery broker configuration via environment variables. Prints skip message if unconfigured, enabling offline notebook execution without external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for Celery/Flower configuration\n",
    "CELERY_BROKER = os.getenv('CELERY_BROKER_URL')\n",
    "FLOWER_URL = os.getenv('FLOWER_URL', 'http://localhost:5555')\n",
    "\n",
    "if not CELERY_BROKER:\n",
    "    print(\"⚠️ Skipping (no Celery configured)\")\n",
    "    print(\"To validate: Check Flower dashboard for 4-8 active workers\")\n",
    "else:\n",
    "    # Expected: Query Flower API /api/workers\n",
    "    # Expected: workers = {\"worker1\": {\"status\": \"active\"}, \"worker2\": {...}, ...}\n",
    "    # Expected: 4 <= len(workers) <= 8\n",
    "    print(f\"✓ Celery broker: {CELERY_BROKER}\")\n",
    "    print(f\"  Flower URL: {FLOWER_URL}\")\n",
    "    print(\"Manual verification required: Check Flower for active workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Readiness Check #3: Prometheus Metrics Collecting\n",
    "\n",
    "**Requirement:** Verify Grafana dashboard displays `documents_processed_total` metric\n",
    "\n",
    "**Why Critical:** Quality metrics (corruption rate, duplicate %, drift scores) will extend the existing Prometheus setup. If base metrics aren't collecting, quality metrics can't piggyback on the infrastructure.\n",
    "\n",
    "**Pass Criteria:** Prometheus scraping pipeline metrics; Grafana shows recent data points (last 5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this code does:** Looks for Prometheus URL in environment. Falls back to offline mode if absent, prompting manual Grafana dashboard checks instead of automated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for Prometheus/Grafana configuration\n",
    "PROMETHEUS_URL = os.getenv('PROMETHEUS_URL', 'http://localhost:9090')\n",
    "GRAFANA_URL = os.getenv('GRAFANA_URL', 'http://localhost:3000')\n",
    "\n",
    "if not os.getenv('PROMETHEUS_URL'):\n",
    "    print(\"⚠️ Skipping (no Prometheus configured)\")\n",
    "    print(\"To validate: Check Grafana for 'documents_processed_total' metric\")\n",
    "else:\n",
    "    # Expected: Query Prometheus API /api/v1/query?query=documents_processed_total\n",
    "    # Expected: result = {\"data\": {\"result\": [{\"value\": [timestamp, \"1234\"]}]}}\n",
    "    # Expected: int(result[\"data\"][\"result\"][0][\"value\"][1]) > 0\n",
    "    print(f\"✓ Prometheus: {PROMETHEUS_URL}\")\n",
    "    print(f\"  Grafana: {GRAFANA_URL}\")\n",
    "    print(\"Manual verification: Check Grafana for documents_processed_total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Readiness Check #4: Minimum Dataset Size\n",
    "\n",
    "**Requirement:** Verify Pinecone shows 1,000+ vectors indexed\n",
    "\n",
    "**Why Critical:** Quality analysis (detecting duplicates, measuring drift) requires statistically meaningful dataset sizes. Testing on <100 documents produces unreliable quality signals.\n",
    "\n",
    "**Pass Criteria:** Vector database contains at least 1,000 indexed documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this code does:** Validates Pinecone API credentials exist in environment. Skips external API calls if missing, preserving offline execution while guiding manual dashboard validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for Pinecone configuration\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv('PINECONE_ENVIRONMENT')\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"⚠️ Skipping (no Pinecone API key)\")\n",
    "    print(\"To validate: Check Pinecone dashboard for 1,000+ vectors\")\n",
    "else:\n",
    "    # Expected: from pinecone import Pinecone; pc = Pinecone(api_key=...)\n",
    "    # Expected: index = pc.Index(\"your-index\"); stats = index.describe_index_stats()\n",
    "    # Expected: stats[\"total_vector_count\"] >= 1000\n",
    "    print(f\"✓ Pinecone API key configured\")\n",
    "    print(f\"  Environment: {PINECONE_ENV or 'not set'}\")\n",
    "    print(\"Manual verification: Confirm 1,000+ vectors in Pinecone index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Call-Forward: What M5.3 Will Introduce\n",
    "\n",
    "**Central Question:** \"How do you know you're indexing good quality data?\"\n",
    "\n",
    "Module 5.3 (Data Quality) builds three critical capabilities on top of your production pipeline:\n",
    "\n",
    "### 6.1 Chunk Quality Scoring\n",
    "- **Goal:** Detect corrupted text and extraction errors\n",
    "- **Target Accuracy:** >80% detection rate\n",
    "- **Techniques:** Text entropy analysis, language detection, structural validation\n",
    "- **Output:** Per-chunk quality scores, automated filtering of low-quality data\n",
    "\n",
    "### 6.2 Duplicate Detection\n",
    "- **Goal:** Identify near-duplicate documents before indexing\n",
    "- **Target Performance:** <5% false positive rate\n",
    "- **Techniques:** MinHash signatures, Locality-Sensitive Hashing (LSH)\n",
    "- **Output:** Deduplication reports, similarity clustering\n",
    "\n",
    "### 6.3 Data Drift Monitoring\n",
    "- **Goal:** Alert when document distributions change meaningfully\n",
    "- **Techniques:** Statistical tests (KS test, chi-square), distribution tracking\n",
    "- **Output:** Drift alerts, anomaly detection dashboards\n",
    "- **Integration:** Extends existing Prometheus/Grafana monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**Why This Matters:** Your pipeline can process 5,000 documents efficiently, but without quality checks, you might be indexing corrupted text, duplicates, or degraded data. M5.3 ensures the data quality matches your pipeline performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
