{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bridge L3.M7.1 → L3.M7.2 Readiness Notebook\n\n## Purpose\n\nThis bridge validates your transition from **M7.1 (Distributed Tracing)** to **M7.2 (Application Performance Monitoring)**. \n\nM7.1 gave you request-level visibility: you can see that `openai.generate` took 3.5 seconds in a trace. M7.2 adds function-level visibility: you'll discover which specific function inside that span is the bottleneck. This matters because distributed tracing shows WHAT is slow, but APM reveals WHY — enabling targeted optimizations instead of guesswork."
  },
  {
   "cell_type": "markdown",
   "source": "## Concepts Covered\n\nThis bridge validates four readiness checks (delta from M7.1):\n\n- Jaeger UI accessibility and trace availability\n- 10% sampling rate configuration to avoid APM cost overruns\n- Custom span attributes working (foundation for M7.2's advanced attributes)\n- OpenTelemetry + Datadog dependencies installed for M7.2 integration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## After Completing This Notebook\n\nYou will be able to:\n\n- Confirm Jaeger UI is running and displaying traces from M7.1\n- Verify 10% sampling configuration to prevent M7.2 cost overruns\n- Validate custom span attributes (foundation for M7.2's function-level metadata)\n- Test OpenTelemetry and Datadog SDK imports needed for M7.2 APM",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Context in Track\n\n**Bridge L3.M7.1 → L3.M7.2** within Module 7 (Distributed Tracing & Advanced Observability)\n\nThis 8-10 minute bridge validates your M7.1 distributed tracing setup before advancing to M7.2's application performance monitoring with Datadog APM.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Run Locally\n\n**Windows (PowerShell):**\n```powershell\npowershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook\"\n```\n\n**macOS/Linux:**\n```bash\nPYTHONPATH=$PWD jupyter notebook\n```\n\nThen open this notebook and run all cells sequentially.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: RECAP - What M7.1 Accomplished\n",
    "\n",
    "### Achievement 1: OpenTelemetry Instrumentation Working End-to-End\n",
    "✓ OpenTelemetry SDK configured with OTLP exporter  \n",
    "✓ BatchSpanProcessor exporting every 5 seconds  \n",
    "✓ Jaeger backend running at localhost:16686  \n",
    "✓ Auto-instrumented FastAPI with FastAPIInstrumentor  \n",
    "✓ Manual spans added to RAG pipeline (retrieve, rerank, generate)  \n",
    "\n",
    "**Key Win:** Can now answer \"Where did 4.2 seconds go in THIS specific request?\"\n",
    "\n",
    "### Achievement 2: Trace Visualization in Jaeger UI\n",
    "✓ Deployed Jaeger and viewed first traces  \n",
    "✓ Parent span (POST /query) with child spans for each operation  \n",
    "✓ Span attributes showing llm.model, user_id, cost_usd  \n",
    "✓ Filtering capability (e.g., duration >2s)  \n",
    "\n",
    "### Achievement 3: Sampling Strategy Configured\n",
    "✓ Implemented 10% head-based sampling  \n",
    "✓ Reduced overhead from 20ms → 2ms per request (90% reduction)  \n",
    "✓ Balanced cost vs coverage (1,000 traces/day at 10K requests/day)  \n",
    "\n",
    "### Achievement 4: Production Observability Deployed\n",
    "✓ Every request gets a trace_id  \n",
    "✓ Context propagates through all services  \n",
    "✓ Traces retained for 30 days  \n",
    "\n",
    "**What you gained:** Request-level observability — can trace slow requests from FastAPI → Pinecone → OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Readiness Check #1 - Jaeger Showing Traces\n\n**Checkpoint:** Verify Jaeger UI is accessible and showing traces  \n**Location:** http://localhost:16686  \n**Success Criteria:** ≥10 traces from last hour with waterfall view\n\n**Impact if failing:** M7.2 APM traces won't correlate without working Jaeger (2 hours troubleshooting saved)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Test Jaeger Connectivity\n\nThe following cell attempts an HTTP request to the Jaeger UI endpoint. If Jaeger isn't running, the test gracefully skips with a warning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import requests\n\n# Check if Jaeger UI is accessible\ntry:\n    response = requests.get(\"http://localhost:16686\", timeout=3)\n    if response.status_code == 200:\n        print(\"✓ Jaeger UI accessible at http://localhost:16686\")\n    else:\n        print(f\"⚠️ Jaeger UI returned status {response.status_code}\")\nexcept Exception as e:\n    print(f\"⚠️ Skipping (no Jaeger service): {type(e).__name__}\")\n\n# Expected:\n# ✓ Jaeger UI accessible at http://localhost:16686\n# (Manual verification needed: Open UI and check for ≥10 traces)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Readiness Check #2 - Sampling Configured\n\n**Checkpoint:** Verify sampling rate is set to 10% (0.10)  \n**Location:** Check `tracer.py` or equivalent configuration  \n**Success Criteria:** `TraceIdRatioBased(0.10)` or similar 10% sampling configured\n\n**Impact if failing:** 100% sampling → overwhelmed APM backend + high costs (unexpected bills avoided)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Search for Tracer Configuration Files\n\nThe following cell scans your project for tracer configuration files (e.g., `tracer.py`) to help you locate where sampling is configured.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport glob\n\n# Search for tracer configuration files\ntracer_files = glob.glob(\"**/tracer.py\", recursive=True) + glob.glob(\"**/*trace*.py\", recursive=True)\n\nif tracer_files:\n    print(f\"Found {len(tracer_files)} tracer config file(s):\")\n    for f in tracer_files[:3]:  # Show first 3\n        print(f\"  - {f}\")\n    print(\"⚠️ Manual check needed: Verify TraceIdRatioBased(0.10) in config\")\nelse:\n    print(\"⚠️ No tracer.py found - manual verification required\")\n\n# Expected:\n# Found 1 tracer config file(s): tracer.py\n# ⚠️ Manual check: Verify TraceIdRatioBased(0.10) in config",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Readiness Check #3 - Custom Span Attributes\n\n**Checkpoint:** Verify custom attributes (llm.model, llm.prompt_tokens, llm.completion_tokens) appear in traces  \n**Location:** Jaeger UI → Click on `openai.generate` span → Attributes section  \n**Success Criteria:** Attributes show model name and token counts\n\n**Impact if failing:** M7.2 adds MORE attributes (function names, memory) — if basic attributes don't work, advanced ones won't either (1-2 hours debugging saved)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Print Manual Verification Instructions\n\nThe following cell outputs step-by-step instructions for manually verifying custom span attributes in the Jaeger UI.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate checking span attributes\nprint(\"⚠️ Manual verification required:\")\nprint(\"  1. Open Jaeger UI at http://localhost:16686\")\nprint(\"  2. Select a trace and click on 'openai.generate' span\")\nprint(\"  3. Verify attributes section shows:\")\nprint(\"     - llm.model (e.g., 'gpt-4')\")\nprint(\"     - llm.prompt_tokens (e.g., 1850)\")\nprint(\"     - llm.completion_tokens (e.g., 420)\")\n\n# Expected:\n# ⚠️ Manual verification required: Check Jaeger UI for custom attributes",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Readiness Check #4 - Dependencies Installed\n\n**Checkpoint:** Verify OpenTelemetry and Datadog dependencies are installed  \n**Required packages:** `opentelemetry`, `ddtrace`  \n**Success Criteria:** Import test succeeds without errors\n\n**Impact if failing:** M7.2 uses both OpenTelemetry + Datadog SDK — missing dependencies = import errors (30 min troubleshooting saved)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Test Required Package Imports\n\nThe following cell attempts to import OpenTelemetry and Datadog packages. If packages are missing, it prints installation instructions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test dependency imports\ndeps = {\"opentelemetry\": False, \"ddtrace\": False}\n\ntry:\n    import opentelemetry\n    deps[\"opentelemetry\"] = True\n    print(\"✓ opentelemetry installed\")\nexcept ImportError:\n    print(\"✗ opentelemetry NOT installed (run: pip install opentelemetry-api opentelemetry-sdk)\")\n\ntry:\n    from ddtrace import tracer\n    deps[\"ddtrace\"] = True\n    print(\"✓ ddtrace installed\")\nexcept ImportError:\n    print(\"✗ ddtrace NOT installed (run: pip install ddtrace)\")\n\nif all(deps.values()):\n    print(\"\\n✓ All dependencies ready for M7.2\")\n\n# Expected:\n# ✓ opentelemetry installed\n# ✓ ddtrace installed\n# ✓ All dependencies ready for M7.2",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: CALL-FORWARD - What M7.2 Will Introduce\n\n### The Gap: Traces Show WHAT is Slow, Not WHY\n\n**Current state (M7.1):**  \nYou see `openai.generate` took 3,500ms in Jaeger — but you don't know WHY.\n\n**Questions you CAN'T answer yet:**\n- Is 3.5s normal for GPT-4 with 1850 tokens? Or is this abnormal?\n- Are you sending redundant data in the prompt (wasting tokens)?\n- Which FUNCTION inside `generate_response()` is the bottleneck?\n- Is there a memory leak degrading performance over time?\n\n### M7.2: Application Performance Monitoring with Datadog APM\n\n**Three capabilities you'll gain:**\n\n**1. Function-Level Performance Breakdown**\n- See which function inside `openai.generate` span is slow\n- Automatic profiling: no manual instrumentation needed\n- Example: Discover `serialize_prompt()` takes 800ms (can optimize serialization)\n\n**2. Memory Leak Detection**\n- Monitor heap growth over time\n- Identify objects not being garbage collected\n- Example: Find that `document_cache` dict grows unbounded (add eviction policy)\n\n**3. Database Query Optimization**\n- See exact SQL queries being executed\n- Identify N+1 query problems\n- Example: Discover 50 individual SELECT queries (should be 1 JOIN)\n\n### M7.2 Setup: One-Line Integration\n```bash\nddtrace-run python main.py\n```\n- Automatic instrumentation: FastAPI, Redis, OpenAI, Pinecone\n- Correlates with your existing OpenTelemetry traces\n- Function-level flame graphs showing CPU time\n\n### Driving Question for M7.2:\n**\"Traces show `openai.generate` took 3.5 seconds. Which FUNCTION inside that span is the bottleneck?\"**\n\nReady to dive deeper into code-level observability!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}